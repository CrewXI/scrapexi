# Codebase Review: ScrapeXi (DIY AgentQL Alternative)

## 1. System Architecture & Service Dependencies
The project is a multi-tier web scraping platform designed to mimic AgentQL functionality using local/hosted LLMs (Gemini).

### Components:
- **Frontend (Static HTML/JS)**: A modern, "cyber-styled" dashboard [`frontend/dashboard.html`](frontend/dashboard.html) and landing page [`frontend/index.html`](frontend/index.html).
- **Primary API (FastAPI)**: [`api/index.py`](api/index.py) handles job management, billing (Stripe), authentication (Supabase), and serves the frontend.
- **Dedicated Scraper Service (FastAPI/Docker)**: [`scraper_service/main.py`](scraper_service/main.py) is a standalone microservice for heavy-duty scraping, designed to bypass Vercel's execution limits.
- **Local Extraction Service**: [`api/local_agentql_service.py`](api/local_agentql_service.py) provides a lightweight fallback for local extraction using Gemini REST calls.

### Dependencies:
- **Infrastructure**: Vercel (Hosting), Supabase (Auth/Database), Stripe (Billing).
- **Libraries**: Playwright (Browser Automation), Beautiful Soup (HTML Cleaning), Google Generative AI (Gemini), FastAPI (API Framework).

## 2. Extraction Logic & AI Prompt Strategies
ScrapeXi uses a two-step AI approach:
1. **HTML Cleaning**: Reduces token usage by stripping non-essential tags (scripts, styles, etc.).
2. **LLM Extraction**:
   - Uses Gemini (Flash) to map natural language queries/GraphQL-like schemas to structured JSON.
   - **Pagination**: Implements two strategies:
     - *Pattern Recognition*: Learning URL structures from Page 1/2/3 examples.
     - *Visual Selector Discovery*: Asking the LLM to find the CSS selector for the "Next" button.

### Observations:
- **Token Efficiency**: The cleaning logic in [`api/local_agentql_service.py`](api/local_agentql_service.py) is effective but might lose context for complex layouts.
- **Stealth**: Built-in support for `playwright-stealth` and custom user-agents.

## 3. Frontend Implementation (Dashboard/Studio)
The dashboard is a single-file SPA [`frontend/dashboard.html`](frontend/dashboard.html) using Tailwind CSS and vanilla JavaScript.

### Features:
- **Visual Schema Builder**: A "Wizard" mode for non-technical users to define extraction targets.
- **JSON Editor**: Integrated CodeMirror for advanced GraphQL-style queries.
- **Session Management**: Direct integration with "EditThisCookie" format for bypassing logins via cookie injection.

## 4. Billing & Subscription Integration
- **Quota Enforcement**: Usage is tracked in Megabytes (MB) of extracted JSON.
- **Stripe Webhooks**: Automatically updates user limits in Supabase when payments succeed.
- **Database Schema**: Expects a `profiles` table for usage limits and a `jobs` table for history.

## 5. Recommendations & Potential Issues
1. **Security**: The current implementation warns about not storing passwords but the `ScrapeRequest` model still accepts them. Consider emphasizing Session Cookie usage.
2. **Error Handling**: The JSON cleaning logic [`api/local_agentql_service.py:25`](api/local_agentql_service.py:25) is basic. Truncated LLM responses might still cause parser errors.
3. **Microservice Sync**: Ensure `BROWSER_SERVICE_URL` is correctly configured in production to avoid Vercel's Playwright limitations.
4. **AgentQL Parity**: The system implements `query_data` but doesn't fully replicate the `get_by_prompt` (locator) functionality in the local service as deeply as the Python examples suggest.

---
*Generated by Roo (Architect Mode)*
